{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import datasets, transforms, utils, models\n",
    "from skimage import io, transform, img_as_float\n",
    "from torch.autograd import Variable\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines.utils import concordance_index\n",
    "import tables\n",
    "import csv\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import copy\n",
    "import os\n",
    "from PIL import *\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prognostic_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "     \n",
    "        self.files_list = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if type(idx) == torch.Tensor:\n",
    "                idx = idx.item()\n",
    "        \n",
    "        he_name = os.path.join(self.files_list.iloc[idx, 0])\n",
    "        he_name = he_name.replace('Teresa_TMA_patch', 'Patches')\n",
    "        index_sub = he_name.find('Patches')\n",
    "        he_name = os.path.join('/deepdata/adib/prognostic_study', he_name[index_sub:])\n",
    "        he_name = he_name.replace('\\\\', '/') \n",
    "       \n",
    "\n",
    "        \n",
    "        status = files_list.iloc[idx, 11]\n",
    "        age = files_list.iloc[idx, 2]\n",
    "        sex = files_list.iloc[idx, 3]\n",
    "        os_month = files_list.iloc[idx, 13]\n",
    "\n",
    "        event=0\n",
    "        if status=='deceased':\n",
    "            event=1\n",
    "        \n",
    "        he_image = Image.open( he_name )\n",
    "\n",
    "        he_image = img_as_float(he_image)\n",
    "        \n",
    "        sample = {'input': he_image, 'event': event, 'OS':os_month}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        input, event, os_month = sample['input'], sample['event'], sample['OS']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        input = input.transpose((2, 0, 1))\n",
    "        sample = {'input': input, 'event': event, 'OS':os_month}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuming/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/yuming/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3214: DtypeWarning: Columns (10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "csv_file='/deepdata/adib/prognostic_study/pytorch_code/Patch_clinical_clean_v2.csv'\n",
    "files_list = pd.read_csv(csv_file)\n",
    "prognostic_dataset= Prognostic_Dataset( csv_file, transform=ToTensor())\n",
    "#import nonechucks as nc\n",
    "#prognostic_dataset = nc.SafeDataset(prognostic_dataset)\n",
    "dataloader = DataLoader(prognostic_dataset, batch_size=280,shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class survresnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(survresnet, self).__init__()\n",
    "        cox_in_dim=4\n",
    "        label_dim = 1\n",
    "        use_pretrained=True \n",
    "        feature_extract = False\n",
    "\n",
    "        PATH=\"/deepdata/adib/prognostic_study/pytorch_code/trained_model/trainedResnet.pth\"\n",
    "        model_ft = models.resnet101(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, 4)\n",
    "        input_size = 224\n",
    "        model_ft.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "        self.resnet=model_ft\n",
    "        self.coxnet = nn.Sequential(nn.Linear(cox_in_dim, label_dim),nn.Softplus())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_d = None\n",
    "        \n",
    "        code = self.resnet(x)\n",
    "        lbl_pred = self.coxnet(code)\n",
    "        \n",
    "        return x_d, code, lbl_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def accuracy_cox(hazards, labels):\n",
    "    # This accuracy is based on estimated survival events against true survival events\n",
    "    hazardsdata = hazards.cpu().numpy().reshape(-1)\n",
    "    median = np.median(hazardsdata)\n",
    "    hazards_dichotomize = np.zeros([len(hazardsdata)], dtype=int)\n",
    "    hazards_dichotomize[hazardsdata > median] = 1\n",
    "    labels = labels.data.cpu().numpy()\n",
    "    correct = np.sum(hazards_dichotomize == labels)\n",
    "    return correct / len(labels)\n",
    "\n",
    "def cox_log_rank(hazards, labels, survtime_all):\n",
    "    hazardsdata = hazards.cpu().numpy().reshape(-1)\n",
    "    median = np.median(hazardsdata)\n",
    "    hazards_dichotomize = np.zeros([len(hazardsdata)], dtype=int)\n",
    "    hazards_dichotomize[hazardsdata > median] = 1\n",
    "    survtime_all = survtime_all.data.cpu().numpy().reshape(-1)\n",
    "    idx = hazards_dichotomize == 0\n",
    "    labels = labels.data.cpu().numpy()\n",
    "    T1 = survtime_all[idx]\n",
    "    T2 = survtime_all[~idx]\n",
    "    E1 = labels[idx]\n",
    "    E2 = labels[~idx]\n",
    "    results = logrank_test(T1, T2, event_observed_A=E1, event_observed_B=E2)\n",
    "    pvalue_pred = results.p_value\n",
    "    return(pvalue_pred)\n",
    "    \n",
    "def CIndex(hazards, labels, survtime_all):\n",
    "    labels = labels.data.cpu().numpy()\n",
    "    concord = 0.\n",
    "    total = 0.\n",
    "    N_test = labels.shape[0]\n",
    "    labels = np.asarray(labels, dtype=bool)\n",
    "    for i in range(N_test):\n",
    "        if labels[i] == 1:\n",
    "            for j in range(N_test):\n",
    "                if survtime_all[j] > survtime_all[i]:\n",
    "                    total = total + 1\n",
    "                    if hazards[j] < hazards[i]: concord = concord + 1\n",
    "                    elif hazards[j] < hazards[i]: concord = concord + 0.5\n",
    "\n",
    "    return(concord/total)\n",
    "    \n",
    "def CIndex_lifeline(hazards, labels, survtime_all):\n",
    "    labels = labels.data.cpu().numpy()\n",
    "    hazards = hazards.cpu().numpy().reshape(-1)\n",
    "    return(concordance_index(survtime_all, -hazards, labels))\n",
    "        \n",
    "def frobenius_norm_loss(a, b):\n",
    "    loss = torch.sqrt(torch.sum(torch.abs(a-b)**2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, num_epochs, batch_size, learning_rate, dropout_rate,\n",
    "                        lambda_1, measure, verbose):\n",
    "    \n",
    "\n",
    "   # dataloader = DataLoader(prognostic_dataset, batch_size=batch_size,shuffle=False, num_workers=0)\n",
    "    \n",
    "    cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed_all(666)\n",
    "    torch.manual_seed(666)\n",
    "    random.seed(666)\n",
    "    \n",
    "#     model = survresnet()\n",
    "#     device=torch.device('cuda:0')\n",
    "#     model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "    \n",
    "    lr = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lr_lambda=LambdaLR(num_epochs, 0, 10).step\n",
    "    )\n",
    "    \n",
    "    c_index_list = {}\n",
    "    c_index_list = []\n",
    "    loss_nn_all = []\n",
    "    pvalue_all = []\n",
    "    c_index_all = []\n",
    "    acc_train_all = []\n",
    "    c_index_best = 0\n",
    "    code_output = None\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        lr.step()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(\"learning rate:\" + str(param_group['lr']))\n",
    "        model.train()\n",
    "        lbl_pred_all = None\n",
    "        lbl_all = None\n",
    "        survtime_all = None\n",
    "        code_final = None\n",
    "        loss_nn_sum = 0\n",
    "        iter = 0\n",
    "        gc.collect()\n",
    "        for iteration, batch in enumerate(dataloader):\n",
    "            data, lbl, survtime = batch['input'], batch['event'], batch['OS']\n",
    "            optimizer.zero_grad() # zero the gradient buffer\n",
    "            graph = data\n",
    "            \n",
    "            \n",
    "            graph = graph.to(device, dtype=torch.float)\n",
    "            lbl = lbl.to(device)\n",
    "            # ===================forward=====================\n",
    "            output, code, lbl_pred = model(graph)\n",
    "            \n",
    "            if iter == 0:\n",
    "                lbl_pred_all = lbl_pred\n",
    "                survtime_all = survtime\n",
    "                lbl_all = lbl\n",
    "                code_final = code\n",
    "            else:\n",
    "                lbl_pred_all = torch.cat([lbl_pred_all, lbl_pred])\n",
    "                lbl_all = torch.cat([lbl_all, lbl])\n",
    "                survtime_all = torch.cat([survtime_all, survtime])\n",
    "                code_final = torch.cat([code_final, code])\n",
    "            current_batch_len = len(survtime)\n",
    "            R_matrix_train = np.zeros([current_batch_len, current_batch_len], dtype=int)\n",
    "            for i in range(current_batch_len):\n",
    "                for j in range(current_batch_len):\n",
    "                    R_matrix_train[i,j] = survtime[j] >= survtime[i]\n",
    "        \n",
    "            train_R = torch.FloatTensor(R_matrix_train)\n",
    "            \n",
    "            train_R = train_R.to(device, dtype=torch.float)\n",
    "            train_ystatus = lbl\n",
    "            \n",
    "            theta = lbl_pred.reshape(-1)\n",
    "            exp_theta = torch.exp(theta)\n",
    "            \n",
    "            loss_nn = -torch.mean( (theta - torch.log(torch.sum( exp_theta*train_R ,dim=1))) * train_ystatus.float() )\n",
    "\n",
    "            l1_reg = None\n",
    "            for W in model.parameters():\n",
    "                if l1_reg is None:\n",
    "                    l1_reg = torch.abs(W).sum()\n",
    "                else:\n",
    "                    l1_reg = l1_reg + torch.abs(W).sum() # torch.abs(W).sum() is equivalent to W.norm(1)\n",
    "            \n",
    "            loss = loss_nn + lambda_1 * l1_reg\n",
    "            if verbose > 2:\n",
    "                print(\"\\nloss_nn: %.4f, L1: %.4f\" % (loss_nn, lambda_1 * l1_reg))\n",
    "            loss_nn_sum = loss_nn_sum + loss_nn.data.item()\n",
    "            # ===================backward====================\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            iter += 1\n",
    "#             torch.cuda.empty_cache()\n",
    "       # code_final_4_original_data = code_final.data.cpu().numpy()\n",
    "        \n",
    "        if measure or epoch == (num_epochs - 1):\n",
    "            acc_train = accuracy_cox(lbl_pred_all.data, lbl_all)\n",
    "            pvalue_pred = cox_log_rank(lbl_pred_all.data, lbl_all, survtime_all)\n",
    "            c_index = CIndex_lifeline(lbl_pred_all.data, lbl_all, survtime_all)\n",
    "            \n",
    "            c_index_list.append(c_index)\n",
    "            if c_index > c_index_best:\n",
    "                c_index_best = c_index\n",
    "            #    code_output = code_final_4_original_data\n",
    "            if verbose > 0:\n",
    "                print('\\n[Training]\\t loss (nn):{:.4f}'.format(loss_nn_sum),\n",
    "                      'c_index: {:.4f}, p-value: {:.3e}'.format(c_index, pvalue_pred))\n",
    "                torch.save(model.state_dict(), Save_model_path)\n",
    "            pvalue_all.append(pvalue_pred)\n",
    "            c_index_all.append(c_index)\n",
    "            loss_nn_all.append(loss_nn_sum)\n",
    "            acc_train_all.append(acc_train)\n",
    "    return(model, loss_nn_all, pvalue_all, c_index_all, c_index_list, acc_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd();\n",
    "save_path = os.path.join(cwd, 'Saved model')\n",
    "Save_model_path=os.path.join(save_path,'survivalnet.pth')\n",
    "Tensor = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): survresnet(\n",
       "    (resnet): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (6): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (7): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (8): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (9): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (10): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (11): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (12): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (13): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (14): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (15): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (16): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (17): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (18): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (19): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (20): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (21): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (22): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "      (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
       "    )\n",
       "    (coxnet): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=1, bias=True)\n",
       "      (1): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "path = os.path.join(cwd, 'Saved model', 'survivalnet.pth')\n",
    "state_dict = torch.load(path, map_location=torch.device('cuda:1'))\n",
    "device = torch.device('cuda:1')\n",
    "gpu_ids = [1, 2, 3, 4, 5, 6, 7]\n",
    "torch.cuda.set_device(gpu_ids[0])\n",
    "model = survresnet()\n",
    "model = torch.nn.DataParallel(model, device_ids=gpu_ids)\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), Save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 350\n",
    "lr = 0.001\n",
    "verbose = 0\n",
    "measure_while_training = True\n",
    "dropout_rate = 0\n",
    "lambda_1 = 1e-5 # L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):8485.4032 c_index: 0.6095, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [1:53:11<186:46:14, 6791.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-44bda412d944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                                                                               \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                                               \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                                                                               measure=True, verbose=1)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b0bafe5386a4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, num_epochs, batch_size, learning_rate, dropout_rate, lambda_1, measure, verbose)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                     \u001b[0mR_matrix_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurvtime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msurvtime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mtrain_R\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_matrix_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 70 per gpu\n",
    "model, loss_nn_all, pvalue_all, c_index_all, c_index_list, acc_train_all, code_output = train(dataloader, num_epochs, \n",
    "                                                                                              batch_size, \n",
    "                                                                                              lr, dropout_rate,lambda_1,\n",
    "                                                                                              measure=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):10852.2202 c_index: 0.6704, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [1:45:56<174:47:30, 6356.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-44bda412d944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                                                                               \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                                               \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                                                                               measure=True, verbose=1)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-b0bafe5386a4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, num_epochs, batch_size, learning_rate, dropout_rate, lambda_1, measure, verbose)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mloss_nn_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_nn_sum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# ===================backward====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 50 per gpu\n",
    "model, loss_nn_all, pvalue_all, c_index_all, c_index_list, acc_train_all, code_output = train(dataloader, num_epochs, \n",
    "                                                                                              batch_size, \n",
    "                                                                                              lr, dropout_rate,lambda_1,\n",
    "                                                                                              measure=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):12636.8156 c_index: 0.6876, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [1:45:38<174:19:02, 6338.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):12227.6670 c_index: 0.7690, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 2/100 [3:24:27<169:12:32, 6215.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):11978.9867 c_index: 0.8132, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 3/100 [5:04:24<165:42:45, 6150.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):11782.1280 c_index: 0.8265, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 4/100 [6:43:14<162:14:17, 6083.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):11639.5710 c_index: 0.8391, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 5/100 [8:21:51<159:13:59, 6034.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):11532.9887 c_index: 0.8421, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 6/100 [10:02:15<157:28:24, 6030.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):11451.2215 c_index: 0.8503, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 7/100 [11:41:31<155:12:59, 6008.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):11361.8173 c_index: 0.8578, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 8/100 [13:20:39<153:05:06, 5990.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):11299.8168 c_index: 0.8544, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 9/100 [15:00:29<151:25:20, 5990.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):11262.4667 c_index: 0.8629, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 10/100 [16:39:26<149:21:15, 5974.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001\n",
      "\n",
      "[Training]\t loss (nn):11185.3270 c_index: 0.8662, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 11/100 [18:18:36<147:31:18, 5967.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.000988888888888889\n",
      "\n",
      "[Training]\t loss (nn):11158.0592 c_index: 0.8670, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 12/100 [19:58:45<146:09:54, 5979.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.0009777777777777777\n",
      "\n",
      "[Training]\t loss (nn):11085.8110 c_index: 0.8741, p-value: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 13/100 [21:37:48<144:14:39, 5968.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.0009666666666666667\n"
     ]
    }
   ],
   "source": [
    "# 40 per gpu\n",
    "model, loss_nn_all, pvalue_all, c_index_all, c_index_list, acc_train_all, code_output = train(dataloader, num_epochs, \n",
    "                                                                                              batch_size, \n",
    "                                                                                              lr, dropout_rate,lambda_1,\n",
    "                                                                                              measure=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv_file='D:\\\\Prognostic_study\\z\\pytorch_code\\\\Patch_clinical_new.csv'\n",
    "files_list = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Prognostic study\\Patches\\survival\\All_A1_14900_rot180_res1.jpeg\n"
     ]
    }
   ],
   "source": [
    "he_name = os.path.join(files_list.iloc[0, 0])\n",
    "print(he_name)\n",
    "pilImg = Image.open( he_name )\n",
    "he_image = img_as_float(pilImg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\All_A5_21000_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\All_A5_21000_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\All_A9_211400_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\All_A9_211400_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\All_A9_211400_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\All_A9_211400_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\All_A9_211400_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\All_A9_211400_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\All_C17_656100_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\All_C17_845640_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\All_C17_845640_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\All_C9_760320_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\All_D3_68134_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\All_D3_68134_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\HPan-Ade170Sur-01-151_H13_114840_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\HPan-Ade170Sur-01-151_H13_114840_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\deceased\\HPan-Ade170Sur-01-151_H9_125356_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_32670_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_32670_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_32670_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_52635_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_55440_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_73260_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_55440_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_55440_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_32670_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_141900_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_55440_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_32670_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_55440_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_55440_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_52635_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_32670_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_73260_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_326040_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_55440_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_141900_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_55440_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I3_326040_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I5_383460_rot180_res1.jpeg\n",
      "Bad file: F:\\Prognostic study\\Patches\\survival\\HPan-Ade170Sur-01_115_I5_383460_rot180_res1.jpeg\n",
      "975142\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "for i in range(len(files_list)):\n",
    "    he_name = os.path.join(files_list.iloc[i, 0])\n",
    "    try:\n",
    "      img = Image.open(he_name) # open the image file\n",
    "      img.verify() # verify that it is, in fact an image\n",
    "    except (IOError, SyntaxError) as e:\n",
    "      print('Bad file:', he_name) # print out the names of corrupt files\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
